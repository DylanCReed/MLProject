{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1785053,"sourceType":"datasetVersion","datasetId":1050896}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\n\n# Load and clean the data\nfile_path = '/kaggle/input/80000-steam-games-dataset/steam_data.csv'\nsteam_data = pd.read_csv(file_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T01:00:46.644018Z","iopub.execute_input":"2024-12-02T01:00:46.645204Z","iopub.status.idle":"2024-12-02T01:00:49.909371Z","shell.execute_reply.started":"2024-12-02T01:00:46.645147Z","shell.execute_reply":"2024-12-02T01:00:49.908052Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Parse price column\ndef parse_price_strict(price):\n    if 'free' in str(price).lower():\n        return 0.0\n    match = re.search(r\"\\d+(\\.\\d+)?\", str(price))\n    return float(match.group()) if match else None\n\nsteam_data['price_parsed'] = steam_data['price'].apply(parse_price_strict)\n\n# Parse score percentage\ndef parse_score_percentage(user_reviews):\n    match = re.search(r\"(\\d{1,2})%\", str(user_reviews))\n    return int(match.group(1)) if match else None\n\nsteam_data['score_percentage'] = steam_data['user_reviews'].apply(parse_score_percentage)\n\n# Parse categories\nexception_categories = [\n    \"Co-op\", \"Single-player\", \"LAN\", \"Controller Support\", \"Online Multiplayer\", \"Remote Play\", \n    \"Steam Workshop\", \"Downloadable Content\", \"Steam Cards\", \"Trading Cards\", \"VR\", \"MMORPG\", \"VR Support\",\n    \"PvE\", \"PvP\", \"PVP\", \"PVE\", \"Split Screen\", \"Shared Screen\", \"Split/Shared Screen\", \"Cross Platform Play\"\n]\n\ndef split_by_capitalized_letters(categories):\n    if pd.isna(categories) or not isinstance(categories, str):\n        return []\n    matches = re.findall(r'(?:' + '|'.join(exception_categories) + r')|[A-Z][a-z]*', categories)\n    return matches\n\nsteam_data['parsed_categories'] = steam_data['categories'].apply(split_by_capitalized_letters)\n\n# Generate one-hot encoded categories\nunique_categories = set(cat for categories in steam_data['parsed_categories'] for cat in categories)\ncategory_dummies = pd.DataFrame(\n    {f'category_{category}': steam_data['parsed_categories'].apply(lambda x: 1 if category in x else 0)\n     for category in unique_categories}\n)\nsteam_data = pd.concat([steam_data, category_dummies], axis=1)\n\n# Group developers and publishers\ndeveloper_counts = steam_data['developer'].value_counts()\npublisher_counts = steam_data['publisher'].value_counts()\n\nsteam_data['developer_grouped'] = steam_data['developer'].apply(\n    lambda x: 'Known' if pd.notna(x) and developer_counts.get(x, 0) >= 10 else 'Unknown'\n)\nsteam_data['publisher_grouped'] = steam_data['publisher'].apply(\n    lambda x: 'Known' if pd.notna(x) and publisher_counts.get(x, 0) >= 20 else 'Unknown'\n)\n\n# One-hot encode developer and publisher groups\ndeveloper_dummies = pd.get_dummies(steam_data['developer_grouped'], prefix='developer', drop_first=True)\npublisher_dummies = pd.get_dummies(steam_data['publisher_grouped'], prefix='publisher', drop_first=True)\n\n# Drop rows with missing values in key columns\nsteam_data_cleaned = steam_data.dropna(subset=['price_parsed', 'score_percentage', 'categories', 'developer', 'publisher'])\nsteam_data_cleaned = pd.concat([steam_data_cleaned, developer_dummies, publisher_dummies], axis=1)\n\n# Dynamically validate and drop columns\ncolumns_to_drop = ['category_Cloud', 'category_Tablet', 'category_Achievements', \n                   'category_Full', 'category_Includes', 'category_Profile', 'category_Limited', \n                   'category_Features', 'category_Phone', 'category_Quality', 'category_Stats', \n                   'category_Leaderboards', 'category_Content', 'category_Captions', \n                   'category_Together', 'category_High', 'category_Additional', 'category_Audio', \n                   'category_Partial', 'category_Tycoon', 'category_Requires', 'category_Shared', \n                   'category_Rocking', 'category_Frog', 'category_Riders', 'category_Grow', \n                   'category_Europa', 'category_Man', 'category_Art', 'category_Valve', \n                   'category_Clickteam', 'category_Beyond', 'category_Wrath', 'category_Anti', \n                   'category_Cheat', 'category_Special', 'category_In', 'category_Dungeon', \n                   'category_Roller', 'category_Lethal', 'category_Fisherman']\n\n# Filter only existing columns\ncolumns_to_drop = [col for col in columns_to_drop if col in steam_data_cleaned.columns]\nsteam_data_cleaned = steam_data_cleaned.drop(columns=columns_to_drop)\n\n# Include all categories in the feature set\ncategory_columns = [f'category_{category}' for category in unique_categories if f'category_{category}' in steam_data_cleaned.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T01:00:49.911496Z","iopub.execute_input":"2024-12-02T01:00:49.911852Z","iopub.status.idle":"2024-12-02T01:04:08.722884Z","shell.execute_reply.started":"2024-12-02T01:00:49.911818Z","shell.execute_reply":"2024-12-02T01:04:08.721683Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Prepare features and labels\nX_parts = [\n    steam_data_cleaned[['price_parsed']].to_numpy(),  # Include price\n    steam_data_cleaned[category_columns].to_numpy(),  # Include all category columns\n    developer_dummies.to_numpy(),  # Include developer dummies\n    publisher_dummies.to_numpy()  # Include publisher dummies\n]\nX = np.hstack(X_parts)\ny = steam_data_cleaned['score_percentage'].to_numpy()\n\n# Remove rows where y is NaN\nvalid_indices = ~np.isnan(y)\nX = X[valid_indices]\ny = y[valid_indices]\n\n# Impute missing values and standardize features\nimputer = SimpleImputer(strategy='mean')\nscaler = StandardScaler()\nX_imputed = imputer.fit_transform(X)\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T01:04:08.724106Z","iopub.execute_input":"2024-12-02T01:04:08.724433Z","iopub.status.idle":"2024-12-02T01:04:17.518670Z","shell.execute_reply.started":"2024-12-02T01:04:08.724402Z","shell.execute_reply":"2024-12-02T01:04:17.516937Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Hyperparameter tuning for Random Forest\nrf_params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15],\n}\nrf = RandomForestRegressor(random_state=0)\nrf_grid = GridSearchCV(rf, rf_params, scoring='r2', cv=3, n_jobs=-1)\nrf_grid.fit(X_train, y_train)\n\nbest_rf = rf_grid.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\nprint(f\"Random Forest - Best Params: {rf_grid.best_params_}\")\nprint(f\"MSE: {mse_rf:.2f}, R²: {r2_rf:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T01:04:17.521561Z","iopub.execute_input":"2024-12-02T01:04:17.522009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter tuning for XGBoost\nxgb_params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0]\n}\nxgb = XGBRegressor(random_state=0)\nxgb_grid = GridSearchCV(xgb, xgb_params, scoring='r2', cv=3, n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\n\nbest_xgb = xgb_grid.best_estimator_\ny_pred_xgb = best_xgb.predict(X_test)\nmse_xgb = mean_squared_error(y_test, y_pred_xgb)\nr2_xgb = r2_score(y_test, y_pred_xgb)\nprint(f\"XGBoost - Best Params: {xgb_grid.best_params_}\")\nprint(f\"MSE: {mse_xgb:.2f}, R²: {r2_xgb:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Linear Regression model (no hyperparameters to tune)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\nmse_lr = mean_squared_error(y_test, y_pred_lr)\nr2_lr = r2_score(y_test, y_pred_lr)\nprint(f\"Linear Regression - MSE: {mse_lr:.2f}, R²: {r2_lr:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance for Random Forest\nfeature_names = ['price_parsed'] + category_columns + list(developer_dummies.columns) + list(publisher_dummies.columns)\nfeature_importances_rf = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': best_rf.feature_importances_\n}).sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(25, 8))\nsns.barplot(data=feature_importances_rf.head(25), x='Importance', y='Feature')\nplt.title('Top 25 Most Important Features - Random Forest')\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance for Random Forest\nfeature_names = ['price_parsed'] + category_columns + list(developer_dummies.columns) + list(publisher_dummies.columns)\nfeature_importances_xgb = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': best_xgb.feature_importances_\n}).sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(25, 8))\nsns.barplot(data=feature_importances_xgb.head(25), x='Importance', y='Feature')\nplt.title('Top 25 Most Important Features - Random Forest')\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}